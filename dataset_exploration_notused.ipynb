{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking an example csv for heart rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex_hr=pd.read_csv('datasets/COVID-19-Wearables/A0KX894_hr.csv')\n",
    "df_ex_hr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex_hr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex_hr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex_hr['datetime'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max heart rate is normally 220 - 20, as the data recorded complies with the standards of Stanford universities, the participants should be around or superior to 20. lets do a quick check on one of the recorded data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_entries = df_ex_hr.isna().sum()\n",
    "illogical_entries = df_ex_hr[(df_ex_hr['heartrate'] < 30)| (df_ex_hr['heartrate'] > 200)].sum()\n",
    "print('Number of NaN entries: ', nan_entries)\n",
    "print('Number of illogical entries: ', illogical_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex_steps=pd.read_csv('datasets/COVID-19-Wearables/A0KX894_hr.csv')\n",
    "df_ex_steps.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex_steps.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex_steps.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex_steps['datetime'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets ittereate over the directories and save a dictionary containing the users name and the corresponding hr and steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_csv = 'datasets/COVID-19-Wearables/'\n",
    "files = [f for f in os.listdir(dir_csv) if f.endswith('.csv')] # we can straight do it with os.listdir\n",
    "# noticed there are some files with user name containing_1 or _2, so we need to put that into consideration\n",
    "#user_name = [f.split('_')[0] if f.count(\"_\")==1 else \"\"for f in files]\n",
    "# will deal with it inside the dictionary better\n",
    "user_name = [f.split('_')[0] for f in files]\n",
    "mult=0\n",
    "for i in user_name:\n",
    "    if(user_name.count(i)) < 1:\n",
    "        print('missing data for user: ', i)\n",
    "    if(user_name.count(i)) > 2:\n",
    "        mult+=1\n",
    "       \n",
    "user_name_unique = list(set(user_name))\n",
    "\n",
    "dict_user_data={}   \n",
    "for i in user_name_unique:\n",
    "    dict_user_data[i] = dict()# I am not concatinating strings, there is a user with multi entries\n",
    "    dict_user_data[i]['hr'] = [f for f in files if f == i+'_hr.csv']\n",
    "    dict_user_data[i]['steps'] = [f for f in files if f == i+'_steps.csv']\n",
    "    s=[f for f in files if f == i+'_sleep.csv']\n",
    "    dict_user_data[i]['sleep'] = s if len(s)!=0 else None\n",
    "    if files.count(i+'_steps_longterm.csv.csv')>0:\n",
    "        dict_user_data[i]['steps_longterm']=i+'_steps_longterm.csv'\n",
    "    if files.count(i+'_hr_longterm.csv.csv')>0:\n",
    "        dict_user_data[i]['hr_longterm']=i+'_hr_longterm.csv'\n",
    "    if files.count(i+'_sleep_longterm.csv.csv')>0:\n",
    "        dict_user_data[i]['sleep_longterm']=i+'_sleep_longterm.csv'\n",
    "\n",
    "import json\n",
    "with open('datasets/dict_user_data.json', 'w') as fp:\n",
    "    json.dump(dict_user_data, fp)\n",
    "    \n",
    "# to visualize all the directory structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_same_user_dict = {}\n",
    "merged_df_same_user_dict_2 = {}\n",
    "\n",
    "for user in dict_user_data.keys():\n",
    "    if dict_user_data[user]['hr'] != []:\n",
    "        df_hr = pd.read_csv(dir_csv + dict_user_data[user]['hr'][0])\n",
    "        df_hr['datetime'] = pd.to_datetime(df_hr['datetime'])\n",
    "        df_hr.set_index('datetime', inplace=True)\n",
    "        df_hr=df_hr.resample('1Min').mean() # resample to 1 min\n",
    "        #df_hr['heartrate'].resample('1min').mean()\n",
    "    if dict_user_data[user]['steps'] != []:\n",
    "        df_steps=pd.read_csv(dir_csv + dict_user_data[user]['steps'][0])\n",
    "        df_steps['datetime'] = pd.to_datetime(df_steps['datetime'])\n",
    "        df_steps.set_index('datetime', inplace=True)\n",
    "        df_steps =df_steps.resample('1Min') \n",
    "    merged_df_same_user_dict[user]=pd.merge(df_hr, df_steps, on=\"datetime\", how=\"left\")\n",
    "    #merged_df_same_user_dict_2[user]=pd.merge(df_hr, df_steps, left_index=True, right_index=True, how=\"outer\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kenerl doesnt work, so dividing the steps into 2 parts\n",
    "for user in dict_user_data.keys():\n",
    "    merged_df_same_user_dict[user].fillna(method='ffill', inplace=True)\n",
    "    merged_df_same_user_dict[user]=merged_df_same_user_dict[user].rename(columns={ \"user_x\": \"user\"})\n",
    "    merged_df_same_user_dict[user]=merged_df_same_user_dict[user].drop(columns=['user_y'], axis=1)\n",
    "    merged_df_same_user_dict[user]=merged_df_same_user_dict[user].dropna()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing for specific instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = merged_df_same_user_dict['A0KX894']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig=plt.figure(figsize=(20,10))\n",
    "ax=fig.add_subplot(1,1,1)\n",
    "ax2=ax.twinx()\n",
    "ax.plot(df_t['heartrate'])\n",
    "ax2.plot(df_t['steps'], color='r')\n",
    "ax.set_ylabel('heartrate')\n",
    "ax2.set_ylabel('steps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "fig=plt.figure(figsize=(20,10))\n",
    "ax=fig.add_subplot(1,1,1)\n",
    "ax2=ax.twinx()\n",
    "ax.scatter(df_t.index,df_t['steps'],color='r',alpha=1) # different unit we know we dont do an ax twinx temporarily just to visualize correlation\n",
    "ax2.scatter(df_t.index,df_t['heartrate'],alpha=0.1)\n",
    "ax.set_ylabel('steps')\n",
    "ax2.set_ylabel('heartrate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sec_t=60 * 24 * 60\n",
    "df_t['totalsec_newday']=pd.to_timedelta(df_t.index.strftime('%H:%M:%S')).total_seconds()\n",
    "df_t['sin_time'] = np.sin(2*np.pi*df_t['totalsec_newday']/sec_t)\n",
    "df_t['cos_time'] = np.cos(2*np.pi*df_t['totalsec_newday']/sec_t)\n",
    "df_t.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.sample(50).plot.scatter('sin_time','cos_time').set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_t.reset_index(level=0)\n",
    "df_test['index1'] = df_t.index\n",
    "corrMatrix = df_test.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can already exclude features due to high correlation but not yet untill merge \n",
    "# sin and cos look promising \n",
    "for user in dict_user_data.keys():\n",
    "    merged_df_same_user_dict[user]['totalsec_newday']=pd.to_timedelta(merged_df_same_user_dict[user].index.strftime('%H:%M:%S')).total_seconds()\n",
    "    merged_df_same_user_dict[user]['sin_time'] = np.sin(2*np.pi*merged_df_same_user_dict[user]['totalsec_newday']/sec_t)\n",
    "    merged_df_same_user_dict[user]['cos_time'] = np.cos(2*np.pi*merged_df_same_user_dict[user]['totalsec_newday']/sec_t)\n",
    "    merged_df_same_user_dict[user]['time'] = merged_df_same_user_dict[user].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.concat(merged_df_same_user_dict.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master[\"user\"].unique() # confirmed all users are in the master df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.to_csv('datasets/df_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_master['user'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('machine')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5945d009ee0301743890d7430330cf9f851036b09bb94721e9fa183962f8f342"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
